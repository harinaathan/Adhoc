{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca4c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "debug=True # set to False for 0 verbose\n",
    "def traces():\n",
    "    \"\"\"debugging assist\"\"\"\n",
    "    from inspect import currentframe, getouterframes, getframeinfo\n",
    "    if debug:\n",
    "        print(\"Stack trace\")\n",
    "        print(\"%15s\"%\"framename\",' : lineno')\n",
    "        frms=getouterframes(currentframe().f_back)\n",
    "        for frm in frms:\n",
    "            print(\"%15s\"%frm.function,' : ',frm.lineno)\n",
    "            if frm.function=='<module>':\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def nulsCount(df):\n",
    "    \"\"\"summarise missing/unexpected values\"\"\"\n",
    "    from pandas import DataFrame\n",
    "    \n",
    "    d2=DataFrame(columns=[\"NULL\",\"NAN\",\"BLANKS\",\"UNEXP\"])\n",
    "    try:\n",
    "        d2[\"NULL\"] = df.isnull().sum().astype('uint32') # check for null values\n",
    "        d2[\"NAN\"]=df.isna().sum().astype('uint32') # check for NaN\n",
    "        d2[\"BLANKS\"]=df.isin([\"\",\" \"]).sum().astype('uint32') # check for blanks\n",
    "        d2[\"UNEXP\"]=df.isin([\"-\",\"?\",\".\",\"NA\",\"N/A\",\"nan\",\"NAN\",\"Nan\",\"Unknown\",\"UNKNOWN\",\"unknown\"]).sum().astype('uint32') # check for other unexpected values\n",
    "    except:\n",
    "        pass\n",
    "    d2=d2.loc[(d2[\"NULL\"]!=0) | (d2[\"NAN\"]!=0) | (d2[\"BLANKS\"]!=0) | (d2[\"UNEXP\"]!=0)] # shortlist for the missing values\n",
    "    \n",
    "    # convert to percentages\n",
    "    d2[\"NULL %\"]=d2[\"NULL\"].apply(lambda x: round(x*100/df.shape[0],2))\n",
    "    d2[\"NAN %\"]=d2[\"NAN\"].apply(lambda x: round(x*100/df.shape[0],2))\n",
    "    d2[\"BLANKS %\"]=d2[\"BLANKS\"].apply(lambda x: round(x*100/df.shape[0],2))\n",
    "    d2[\"UNEXP %\"]=d2[\"UNEXP\"].apply(lambda x: round(x*100/df.shape[0],2))\n",
    "    \n",
    "    # rearrange\n",
    "    d2=d2[[\"NULL\",\"NULL %\",\"NAN\",\"NAN %\",\"BLANKS\",\"BLANKS %\",\"UNEXP\",\"UNEXP %\"]]\n",
    "    \n",
    "    if d2.shape[0]==0:\n",
    "        print('good to go')\n",
    "        return\n",
    "    else:     \n",
    "        return d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a70217",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "class dummies():\n",
    "    \"\"\"to implement encoding without data leak\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"input : dataframe\"\"\"\n",
    "        self.ref={}\n",
    "        self.fitted=False\n",
    "    \n",
    "    def fit(self,df):\n",
    "        \"\"\"Collect required encoding information\"\"\"\n",
    "        cat=list(df.select_dtypes(include='object').columns)\n",
    "        for col in cat:\n",
    "            unq=list(df[col].value_counts().index)\n",
    "            self.ref.update({col:unq})\n",
    "        self.fitted=True\n",
    "        return\n",
    "    \n",
    "    def transform(self,df):\n",
    "        \"\"\"perform encoding\"\"\"\n",
    "        from pandas import to_numeric\n",
    "        df=df.copy()\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"please fit first\")\n",
    "            return\n",
    "        cat=list(self.ref.keys())\n",
    "        for col in cat:\n",
    "            unq=self.ref.get(col)\n",
    "            for i in unq:\n",
    "                df[col+\"_\"+str(i)]=df[col]\n",
    "                df.loc[df[col+\"_\"+str(i)]==i,[col+\"_\"+str(i)]]=1\n",
    "                df.loc[df[col+\"_\"+str(i)]!=1,[col+\"_\"+str(i)]]=0\n",
    "            df.drop(col,axis=1,inplace=True)\n",
    "            df.drop(col+\"_\"+str(unq[i]),axis=1,inplace=True) #drop_first=True\n",
    "        for col in df.select_dtypes(exclude=[int,float]).columns:\n",
    "            df[col]=df[col].astype('float',errors='ignore') # try casting non numeric features to float\n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self,df):\n",
    "        \"\"\"learn and encode\"\"\"\n",
    "        self.fit(df)\n",
    "        df=self.transform(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ff359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "class remap():\n",
    "    def __init__(self):\n",
    "        \"\"\"performs skew correction and z-score standardisation\"\"\"\n",
    "        self.fitted=False\n",
    "        \n",
    "    def fit(self,df,cols=None):\n",
    "        \"\"\"registers stats of the dataframe\"\"\"\n",
    "        from pandas import DataFrame\n",
    "        from numpy import log, sqrt, abs\n",
    "        \n",
    "        self.cols=cols\n",
    "        if self.cols==None:\n",
    "            self.cols=df.columns\n",
    "        df=df[self.cols].copy()\n",
    "        \n",
    "        self.fitting_info=DataFrame(columns=[\"skew\",\"kurt\",\"min\",\"max\",\"reflect\",\"r_min\",\"r_max\",\"mms\",\"log\",\"sqrt\"],\n",
    "                                       index=df.columns)\n",
    "        \n",
    "        # initialise flags\n",
    "        self.fitting_info[\"reflect\"] = False\n",
    "        self.fitting_info[\"mms\"] = False\n",
    "        self.fitting_info[\"log\"] = False\n",
    "        self.fitting_info[\"sqrt\"] = False\n",
    "        \n",
    "        # reocird basic stats\n",
    "        self.fitting_info[\"skew\"] = df.skew()\n",
    "        self.fitting_info[\"kurt\"] = df.kurt()\n",
    "        self.fitting_info[\"min\"] = df.min()\n",
    "        self.fitting_info[\"max\"] = df.max()\n",
    "        \n",
    "        # test need for reflected transforms\n",
    "        collist=list(self.fitting_info.loc[self.fitting_info[\"skew\"]<=-0.75].index)\n",
    "        for col in collist:            \n",
    "            # read basic stats\n",
    "            [cskew,cmin,cmax]=self.fitting_info.loc[col,[\"skew\",\"min\",\"max\"]]\n",
    "            \n",
    "            # reflect\n",
    "            temp_r = cmax+1-df[col]\n",
    "            cmin=temp_r.min()\n",
    "            cmax=temp_r.max()\n",
    "            self.fitting_info.loc[col,[\"r_min\",\"r_max\"]]=[cmin,cmax]\n",
    "            # scale between 0-500\n",
    "            temp_r_mms = (temp_r-cmin)*500/(cmax-cmin)\n",
    "            self.fitting_info.loc[col,[\"mms_min\",\"mms_max\"]]=[temp_r_mms.min(),temp_r_mms.max()]\n",
    "            # scaled log tranform\n",
    "            temp_r_mms_l = (temp_r_mms+1).apply(log)\n",
    "            # scaled sqrt tranform\n",
    "            temp_r_mms_s = temp_r_mms.apply(sqrt)\n",
    "            # plain log tranform\n",
    "            temp_r_l = (temp_r+1).apply(log)\n",
    "            # plain sqrt tranform\n",
    "            temp_r_s = temp_r.apply(sqrt)\n",
    "            # transformed skews\n",
    "            t_skew = abs([temp_r_l.skew(),temp_r_s.skew(),temp_r_mms_l.skew(),temp_r_mms_s.skew()])\n",
    "            # register flags\n",
    "            if round(min(t_skew),2)<round(abs(cskew),2):\n",
    "                self.fitting_info.loc[col,\"reflect\"]=True\n",
    "                if min(t_skew)==t_skew[0]:\n",
    "                    self.fitting_info.loc[col,\"log\"]=True\n",
    "                    df[col]=temp_r_l\n",
    "                elif min(t_skew)==t_skew[1]:\n",
    "                    self.fitting_info.loc[col,\"sqrt\"]=True\n",
    "                    df[col]=temp_r_s\n",
    "                elif min(t_skew)==t_skew[2]:\n",
    "                    self.fitting_info.loc[col,[\"log\",\"mms\"]]=[True,True]\n",
    "                    df[col]=temp_r_mms_l\n",
    "                elif min(t_skew)==t_skew[3]:\n",
    "                    self.fitting_info.loc[col,[\"sqrt\",\"mms\"]]=[True,True]\n",
    "                    df[col]=temp_r_mms_s                \n",
    "        \n",
    "        # test need for plain transforms\n",
    "        collist=list(self.fitting_info.loc[self.fitting_info[\"skew\"]>=0.75].index)\n",
    "        for col in collist:            \n",
    "            # read basic stats\n",
    "            [cskew,cmin,cmax]=self.fitting_info.loc[col,[\"skew\",\"min\",\"max\"]]\n",
    "            \n",
    "            # scale between 0-500\n",
    "            temp_mms = (df[col]-cmin)*500/(cmax-cmin)\n",
    "            self.fitting_info.loc[col,[\"mms_min\",\"mms_max\"]]=[temp_mms.min(),temp_mms.max()]\n",
    "            # scaled log tranform\n",
    "            temp_mms_l = (temp_mms+1).apply(log)\n",
    "            # scaled sqrt tranform\n",
    "            temp_mms_s = temp_mms.apply(sqrt)\n",
    "            # plain log tranform\n",
    "            temp_l = (df[col]+1).apply(log)\n",
    "            # plain sqrt tranform\n",
    "            temp_s = df[col].apply(sqrt)\n",
    "            # transformed skews\n",
    "            t_skew = abs([temp_l.skew(),temp_s.skew(),temp_mms_l.skew(),temp_mms_s.skew()])\n",
    "            # register flags\n",
    "            if round(min(t_skew),2)<round(abs(cskew),2):\n",
    "                if min(t_skew)==t_skew[0]:\n",
    "                    self.fitting_info.loc[col,\"log\"]=True\n",
    "                    df[col]=temp_l\n",
    "                elif min(t_skew)==t_skew[1]:\n",
    "                    self.fitting_info.loc[col,\"sqrt\"]=True\n",
    "                    df[col]=temp_s\n",
    "                elif min(t_skew)==t_skew[2]:\n",
    "                    self.fitting_info.loc[col,[\"log\",\"mms\"]]=True\n",
    "                    df[col]=temp_mms_l\n",
    "                elif min(t_skew)==t_skew[3]:\n",
    "                    self.fitting_info.loc[col,[\"sqrt\",\"mms\"]]=[True,True]\n",
    "                    df[col]=temp_mms_s\n",
    "        \n",
    "        # set fitted flag\n",
    "        self.fitted=True             \n",
    "    \n",
    "    def transform(self,df):\n",
    "        \"\"\"perform transforms & scaling\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"please fit remap\")\n",
    "            return\n",
    "        \n",
    "        from pandas import merge\n",
    "        from numpy import log, sqrt, abs\n",
    "        \n",
    "        df_orig=df.copy()\n",
    "        df=df[self.cols].copy()\n",
    "        \n",
    "        for col in df.columns:            \n",
    "            # find min max value\n",
    "            cmin = self.fitting_info.loc[col,\"min\"]\n",
    "            cmax = self.fitting_info.loc[col,\"max\"]\n",
    "            \n",
    "            # 1. reflection\n",
    "            if self.fitting_info.loc[col,\"reflect\"]:\n",
    "                temp = cmax+1-df[col]\n",
    "                df[col] = temp\n",
    "                # update min max\n",
    "                cmin = self.fitting_info.loc[col,\"r_min\"] \n",
    "                cmax = self.fitting_info.loc[col,\"r_max\"]\n",
    "                    \n",
    "            # 2. min max scaling for log / sqrt\n",
    "            if self.fitting_info.loc[col,\"mms\"]:\n",
    "                temp = (df[col]-cmin)*500/(cmax-cmin)\n",
    "                df[col] = temp\n",
    "                # update min max\n",
    "                cmin = self.fitting_info.loc[col,\"mms_min\"] \n",
    "                cmax = self.fitting_info.loc[col,\"mms_max\"]\n",
    "            \n",
    "            # 3. shift data to +ve scale\n",
    "            if cmin<0:\n",
    "                df[col]=df[col]-cmin \n",
    "            if df[col].min()<0: # reconfirm\n",
    "                df[col]=df[col]-df[col].min()\n",
    "                    \n",
    "            # 4. log transform\n",
    "            if self.fitting_info.loc[col,\"log\"]:\n",
    "                df[col]=(df[col]+1).apply(log)\n",
    "                \n",
    "            # 5. sqrt transform\n",
    "            if self.fitting_info.loc[col,\"sqrt\"]:\n",
    "                df[col]=df[col].apply(sqrt)\n",
    "                \n",
    "            # 6. reverse Reflection\n",
    "            if self.fitting_info.loc[col,\"reflect\"]:\n",
    "                temp = log(cmax)+1-df[col]\n",
    "                df[col] = temp\n",
    "            \n",
    "            # find skew\n",
    "            self.fitting_info.loc[col,\"trans_skew\"]=df[col].skew()\n",
    "        \n",
    "        # find scaled skew\n",
    "        self.fitting_info[\"trans_scaled_skew\"]=df.skew()\n",
    "            \n",
    "        df_orig.drop(self.cols,axis=1,inplace=True)\n",
    "        df=merge(df_orig,df,left_index=True,right_index=True,how='inner')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def fit_transform(self,df):\n",
    "        \"\"\"fit, remap\"\"\"\n",
    "        self.fit(df)\n",
    "        df=self.transform(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b86f587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "class pandaPoly():\n",
    "    \"\"\"PolynomialFeatures extraction and returns Pandas DataFrame\"\"\"\n",
    "    \n",
    "    def __init__(self,degree=2, interaction_only=False, include_bias=False):\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        self.poly = PolynomialFeatures(degree, interaction_only, include_bias)\n",
    "        self.fitted=False\n",
    "    \n",
    "    def fit(self,df):\n",
    "        self.poly.fit(df)\n",
    "        self.fitted=True\n",
    "    \n",
    "    def transform(self,df):\n",
    "        if self.fitted:\n",
    "            from pandas import DataFrame, merge\n",
    "            df=df.copy()\n",
    "            d2=DataFrame(self.poly.transform(df),index=df.index)\n",
    "            d2=merge(df,d2,left_index=True,right_index=True)\n",
    "            return d2\n",
    "        else:\n",
    "            raise ValueError(\"please fit pandaPoly\")\n",
    "    \n",
    "    def fit_transform(self,df):\n",
    "        self.fit(df)\n",
    "        df=self.transform(df)\n",
    "        return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a8013",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "class pandaCluster():\n",
    "    \"\"\"performs KMeans Clustering and returnd Pandas DataFrame with cluster encoded columns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.fitted=False\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        \n",
    "        # models\n",
    "        self.scl = StandardScaler()\n",
    "        self.dum = dummies()\n",
    "        \n",
    "    def clusterRank(self,df,max_clusters=10):\n",
    "        \n",
    "        from pandas import DataFrame\n",
    "        from sklearn.cluster import KMeans\n",
    "        from scipy.spatial.distance import cdist\n",
    "    \n",
    "        clusters = range(2,max_clusters)\n",
    "        self.elbow=DataFrame(columns=['n_clusters','distortion','slope','slope_delta'])\n",
    "        self.best=DataFrame(columns=['rank','n_clusters','distortion','slope_delta'],index=[1,2])\n",
    "        meanDistortions=[]\n",
    "\n",
    "        # run clustering and measure distortions\n",
    "        for k in clusters:\n",
    "            model=KMeans(n_clusters=k)\n",
    "            model.fit(df)\n",
    "            prediction=model.predict(df)\n",
    "            meanDistortions.append(sum(np.min(cdist(df, model.cluster_centers_, 'euclidean'), axis=1)) / df.shape[0])\n",
    "\n",
    "        # analyse change in distortions\n",
    "        slope=[]\n",
    "        # slope of graph\n",
    "        slope.extend([meanDistortions[i+1]-meanDistortions[i] for i in range(len(meanDistortions)-1)])\n",
    "        slope.append(np.nan)\n",
    "\n",
    "        slope_delta=[np.nan]\n",
    "        # change of slope of graph\n",
    "        slope_delta.extend([slope[i]-slope[i-1] for i in range(1,len(slope)-1)])\n",
    "        slope_delta.append(np.nan)\n",
    "\n",
    "        self.elbow.n_clusters=clusters\n",
    "        self.elbow.distortion=meanDistortions\n",
    "        self.elbow.slope=slope\n",
    "        self.elbow.slope_delta=slope_delta\n",
    "        self.elbow=self.elbow.sort_values(by=['slope_delta','slope'],ascending=False)\n",
    "\n",
    "        # rank number of cluster based on change of slope\n",
    "        for i in range(1,3):\n",
    "            set1=[]\n",
    "            ind=self.elbow.index[i-1]\n",
    "            set1.append(i)\n",
    "            set1.append(ind+2)\n",
    "            set1.append(self.elbow['distortion'].loc[ind])\n",
    "            set1.append(self.elbow['slope_delta'].loc[ind])\n",
    "            self.best.loc[i]=set1\n",
    "\n",
    "        # visualise\n",
    "        import plotly.graph_objects as go\n",
    "        gdata=self.elbow.sort_index().copy()\n",
    "        fig=go.Figure()\n",
    "        # plot optimal cluster numbers\n",
    "        fig.add_trace(go.Scatter(x=self.best.n_clusters, y=self.best.distortion,\n",
    "                                 mode='markers',name='optimal clusters',\n",
    "                                 marker={'size':15,'color':'#FFA15A'},\n",
    "                                 text=self.best['rank'],\n",
    "                                 hovertemplate='<b>OPTIMA %{text}: %{x} clusters)'))\n",
    "        # plot the distortions for cluster range of 2-9\n",
    "        fig.add_trace(go.Scatter(x=gdata.n_clusters, y=gdata.distortion,name='Distortions',\n",
    "                                 marker={'color':'#1F77B4'},\n",
    "                                 hovertemplate='<b>Distortions</b><br>'+\n",
    "                                 'n_clusters: %{x}<br>'+\n",
    "                                 'distortion: %{y:.2f}'))\n",
    "        fig.update_xaxes(title_text=\"n_clusters\")\n",
    "        fig.update_yaxes(title_text=\"distortions\")\n",
    "        fig.update_layout(title=\"Selecting k with the Elbow Method\")\n",
    "        fig.show()\n",
    "    \n",
    "    def fit(self,df):\n",
    "        from pandas import DataFrame\n",
    "        from sklearn.cluster import KMeans\n",
    "        \n",
    "        df=df.copy()\n",
    "        \n",
    "        # scale the data\n",
    "        df= DataFrame(self.scl.fit_transform(df),columns=df.columns,index=df.index)\n",
    "        # optimal cluster choice\n",
    "        self.clusterRank(df)\n",
    "        n=self.best.loc[self.best['rank']==2,'n_clusters'].values[0]\n",
    "        # cluster fitting\n",
    "        self.clt = KMeans(n_clusters=n)\n",
    "        self.clt.fit(df)\n",
    "        # encoder fitting for clusters\n",
    "        pred=DataFrame(self.clt.predict(df),columns=[\"CLUSTER\"],index=df.index,dtype='object')\n",
    "        self.dum.fit(pred)\n",
    "        \n",
    "        self.fitted=True\n",
    "    \n",
    "    def transform(self,df):\n",
    "        if self.fitted:\n",
    "            from pandas import DataFrame, merge\n",
    "            df=df.copy()\n",
    "            dforig=df.copy()\n",
    "            # scale the data\n",
    "            df=DataFrame(self.scl.transform(df),columns=df.columns,index=df.index)\n",
    "            # predict clusters\n",
    "            pred=DataFrame(self.clt.predict(df),columns=[\"CLUSTER\"],index=df.index,dtype='object')\n",
    "            # encode cluster columns\n",
    "            pred=self.dum.transform(pred)\n",
    "            # merge with source\n",
    "            df=merge(dforig,pred,left_index=True,right_index=True)\n",
    "            return df\n",
    "        else:\n",
    "            raise ValueError(\"please fit pandaCluster\")\n",
    "    \n",
    "    def fit_transform(self,df):\n",
    "        self.fit(df)\n",
    "        df=self.transform(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88a666",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def cvSplitter(X,Y,k=10,seed=129):\n",
    "    \"\"\"Splits K folds and returns array of copied dataframes\"\"\"\n",
    "    X=X.copy()\n",
    "    Y=Y.copy()\n",
    "    L=X.shape[0]\n",
    "    from numpy import random, floor\n",
    "    \n",
    "    # seed pseudo random generator\n",
    "    random.seed(seed)\n",
    "    indices=random.choice(X.index,L,False)\n",
    "    sets=[(int(floor(L*(i)/k)),int(floor(L*(i+1)/k))) for i in range(k)]\n",
    "    Xtrains=[]\n",
    "    Xvals=[]\n",
    "    Ytrains=[]\n",
    "    Yvals=[]\n",
    "    ss=0\n",
    "    for i in range(k):\n",
    "        se=int(floor(L*(i+1)/k))\n",
    "        Xvals.append(X.loc[list(indices[ss:se])].copy())\n",
    "        Yvals.append(Y.loc[list(indices[ss:se])].copy())\n",
    "        Xtrains.append(X.loc[list(indices[[j not in indices[ss:se] for j in indices]])].copy())\n",
    "        Ytrains.append(Y.loc[list(indices[[j not in indices[ss:se] for j in indices]])].copy())\n",
    "        ss=se\n",
    "    return Xtrains,Ytrains,Xvals,Yvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124a1d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to /media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "class SCFS():\n",
    "    \"\"\"https://www.frontiersin.org/articles/10.3389/fgene.2021.684100/full\n",
    "    Reference article for feature scoring\n",
    "    SCFS (Standard deviation and Cosine similarity based Feature Selection)\n",
    "    Credits to: Juanying Xie, Mingzhao Wang, Shengquan Xu, Zhao Huang and Philip W. Grant\"\"\"\n",
    "    \n",
    "    def __init__(self,kind='exp',threshold='auto'):\n",
    "        \"\"\"kind = {'exp','reciprocal','anti-similarity'} default='exp'\n",
    "        threshold = {'auto', float between 0.0 and 1.0} default='auto'\"\"\"\n",
    "        \n",
    "        self.kind=kind\n",
    "        self.verbose=1\n",
    "        \n",
    "        autoThresh={'exp':0.2,'reciprocal':0,'anti-similarity':0.2}\n",
    "        if threshold=='auto':\n",
    "            self.threshold=autoThresh.get(kind)\n",
    "        else:\n",
    "            self.threshold=threshold\n",
    "\n",
    "        self.fitted=False\n",
    "        \n",
    "    def discernibility(self):\n",
    "        \"\"\"list down the feature discernibility\n",
    "        same as sample standard deviations\"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        m=self.df.shape[0]\n",
    "        self.dis=[np.sqrt(sum((self.df[i]-sum(self.df[i])/m)**2)/(m-1)) for i in self.df.columns]\n",
    "        self.dis=pd.Series(self.dis,index=self.df.columns,dtype=float)\n",
    "    \n",
    "    def cosineSimilarity(self):\n",
    "        \"\"\"populate the cosine similarities (absolute)\"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        self.cosdf=pd.DataFrame(columns=self.df.columns,index=self.df.columns)\n",
    "        for i in self.df.columns:\n",
    "            for j in self.df.columns:\n",
    "                norm_i=np.sqrt(self.df[i].dot(self.df[i]))\n",
    "                norm_j=np.sqrt(self.df[j].dot(self.df[j]))\n",
    "                self.cosdf.loc[i,j] = (np.abs(self.df[i].dot(self.df[j])))/(norm_i*norm_j)\n",
    "                \n",
    "    def independence(self):\n",
    "        \"\"\"evaluate the feature independance\"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        dismaxarg=self.dis.index[np.argmax(self.dis)]\n",
    "        self.ind=pd.Series(index=self.df.columns,dtype=float)\n",
    "\n",
    "        for i in self.df.columns:\n",
    "            if self.dis[i] == self.dis[dismaxarg]: # for feature with max stddev\n",
    "                if self.kind == 'exp':\n",
    "                    self.ind[i] = np.exp(max(-self.cosdf.loc[i]))\n",
    "                elif self.kind == 'reciprocal':\n",
    "                    self.ind[i] = max(1/self.cosdf.loc[i])\n",
    "                elif self.kind == 'anti-similarity':\n",
    "                    self.ind[i] = max(1-self.cosdf.loc[i])\n",
    "            else:\n",
    "                if self.kind == 'exp':\n",
    "                    self.ind[i] = np.exp(min(-self.cosdf.loc[i,self.dis[self.dis>self.dis[i]].index]))\n",
    "                elif self.kind == 'reciprocal':\n",
    "                    self.ind[i] = min(1/self.cosdf.loc[i,self.dis[self.dis>self.dis[i]].index])\n",
    "                elif self.kind == 'anti-similarity':\n",
    "                    self.ind[i] = min(1-self.cosdf.loc[i,self.dis[self.dis>self.dis[i]].index])\n",
    "                    \n",
    "    def fit(self,df):\n",
    "        \"\"\"evaluate feature scores of df\"\"\"\n",
    "        self.df=df.copy()\n",
    "        \n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        \n",
    "        mima=MinMaxScaler(feature_range=(-1,1))\n",
    "        self.df = pd.DataFrame(mima.fit_transform(self.df),\n",
    "                           columns=self.df.columns,index=self.df.index)\n",
    "        \n",
    "        self.discernibility()\n",
    "        self.cosineSimilarity()\n",
    "        self.independence()\n",
    "        \n",
    "        self.fscore=self.dis.mul(self.ind)\n",
    "        self.fitted=True\n",
    "        \n",
    "        if self.verbose!=0:\n",
    "            import plotly.graph_objects as go\n",
    "            # lets review the feature scores\n",
    "            fig=go.Figure()\n",
    "            gdata=self.fscore.sort_values()\n",
    "            fig.add_trace(go.Scatter(x=gdata.index, y=gdata,name='feature score'))\n",
    "            fig.update_xaxes(title=\"features-->\")\n",
    "            fig.update_yaxes(title=\"scores-->\")\n",
    "            fig.show()\n",
    "\n",
    "            fig=go.Figure()\n",
    "            fig.add_trace(go.Scatter(x=scfs.dis,y=scfs.ind,mode='markers',name='discernibility vs independence'))\n",
    "            fig.update_xaxes(title=\"discernibility-->\")\n",
    "            fig.update_yaxes(title=\"independence-->\")\n",
    "            fig.show()\n",
    "        \n",
    "    def fit_transform(self,df):\n",
    "        \"\"\"interatively reduce features\"\"\"\n",
    "        import gc\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        \n",
    "        self.verbose=0\n",
    "        self.fit(df)        \n",
    "        logscore=np.log(self.fscore)\n",
    "        flag=logscore.min()\n",
    "        \n",
    "        while flag<self.threshold:\n",
    "            gc.collect()\n",
    "            len(gc.get_objects());\n",
    "            \n",
    "            ind=logscore.argmin()\n",
    "            feat=self.fscore.index[ind]\n",
    "            self.df.drop(feat,axis=1,inplace=True)\n",
    "            self.fit(self.df)\n",
    "            logscore=np.log(self.fscore)\n",
    "            flag=logscore.min()\n",
    "        \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f661cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7486a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef275b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b16c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile -a '/media/harinaathan/Barracuda/HARI SAMYNAATH S/Anaconda_workspace/GLAIML_course/myRepo.py'\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "# defining a function to report classification metrics\n",
    "def classReporter(Y_train, pred_train, Y_test, pred_test,model_name):\n",
    "    \"\"\"Classification report\n",
    "    logs test scores to global dataframe named scoreLog\n",
    "    the scoreLog (with any previous scores) will be displayed\n",
    "    also displays confusion matrices of current instance of arguments\n",
    "    ---------------------------------------------------------------------------\n",
    "    Y_train ==> TRUE classes used for training (pandas series object or numpy array of 1-D)\n",
    "    pred_train ==> PREDICTION on training data (pandas series object or numpy array of 1-D)\n",
    "    Y_test ==> TRUE classes to be used for testing (pandas series object or numpy array of 1-D)\n",
    "    pred_test ==> PREDICTION on test data (pandas series object or numpy array of 1-D)\n",
    "    model_name ==> str name for current model, to be used as index for scoreLog\n",
    "    ---------------------------------------------------------------------------\n",
    "    \"\"\"\n",
    "    from sklearn import metrics\n",
    "    import plotly.figure_factory as ff\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    global scoreLog\n",
    "    \n",
    "    classes=list(Y_test.unique())\n",
    "    cols=[\"accuracy\"]\n",
    "    cols.extend([\"precision_\"+str(classes[i]) for i in range(len(classes))])\n",
    "    cols.extend([\"recall_\"+str(classes[i]) for i in range(len(classes))])\n",
    "    cols.extend([\"fscore_\"+str(classes[i]) for i in range(len(classes))])\n",
    "    \n",
    "    try:\n",
    "        type(scoreLog)\n",
    "    except:\n",
    "        scoreLog=pd.DataFrame(columns=cols)\n",
    "    \n",
    "    #metrics based on training set\n",
    "    #confusion matrix\n",
    "    z=pd.DataFrame(metrics.confusion_matrix(Y_train, pred_train))\n",
    "    fig1=ff.create_annotated_heatmap(np.array(z),annotation_text=np.array(z),\n",
    "                                    x=list(np.sort(np.unique(Y_train))),y=list(np.sort(np.unique(Y_train))),\n",
    "                                    colorscale='Mint',font_colors = ['grey','white'],name=\"TRAINING SET\",\n",
    "                                    hovertemplate=\"Prediction: %{x:d}<br>True: %{y:d}<br>Count: %{z:d}\")\n",
    "    fig1.update_layout(height=350,width=350)\n",
    "    fig1.update_xaxes(title_text=\"PREDICTED (TRAINING SET) - \"+model_name)\n",
    "    fig1.update_yaxes(title_text=\"TRUE\",tickangle=270)\n",
    "    \n",
    "    #scores\n",
    "    score=[metrics.accuracy_score(Y_train,pred_train)]\n",
    "    score.extend(metrics.precision_score(Y_train,pred_train,labels=classes,average=None))\n",
    "    score.extend(metrics.recall_score(Y_train,pred_train,labels=classes,average=None))\n",
    "    score.extend(metrics.f1_score(Y_train,pred_train,labels=classes,average=None))\n",
    "    scoreLog=scoreLog.append(pd.DataFrame(score,index=cols,columns=[model_name+\"_training\"]).T)\n",
    "    \n",
    "    #metrics based on test set\n",
    "    #confusion matrix\n",
    "    z=pd.DataFrame(metrics.confusion_matrix(Y_test, pred_test))\n",
    "    fig2=ff.create_annotated_heatmap(np.array(z),annotation_text=np.array(z),\n",
    "                                    x=list(np.sort(np.unique(Y_test))),y=list(np.sort(np.unique(Y_test))),\n",
    "                                    colorscale='Mint',font_colors = ['grey','white'],name=\"TEST SET\",\n",
    "                                    hovertemplate=\"Prediction: %{x:d}<br>True: %{y:d}<br>Count: %{z:d}\")\n",
    "    fig2.update_layout(height=350,width=350)\n",
    "    fig2.update_xaxes(title_text=\"PREDICTED (TEST SET) - \"+model_name)\n",
    "    fig2.update_yaxes(title_text=\"TRUE\",tickangle=270)\n",
    "    \n",
    "    #scores\n",
    "    score=[metrics.accuracy_score(Y_test,pred_test)]\n",
    "    score.extend(metrics.precision_score(Y_test,pred_test,labels=classes,average=None))\n",
    "    score.extend(metrics.recall_score(Y_test,pred_test,labels=classes,average=None))\n",
    "    score.extend(metrics.f1_score(Y_test,pred_test,labels=classes,average=None))\n",
    "    scoreLog=scoreLog.append(pd.DataFrame(score,index=cols,columns=[model_name+\"_test\"]).T)\n",
    "    \n",
    "    # merge both confusion matrix heatplots\n",
    "    fig=make_subplots(rows=1,cols=2,horizontal_spacing=0.05)\n",
    "    fig.add_trace(fig1.data[0],row=1,col=1)#,name=\"training data\")\n",
    "    fig.add_trace(fig2.data[0],row=1,col=2)#,name=\"test data\")\n",
    "\n",
    "    annot1 = list(fig1.layout.annotations)\n",
    "    annot2 = list(fig2.layout.annotations)\n",
    "    for k  in range(len(annot2)):\n",
    "        annot2[k]['xref'] = 'x2'\n",
    "        annot2[k]['yref'] = 'y2'\n",
    "    fig.update_layout(annotations=annot1+annot2) \n",
    "    fig.layout.xaxis.update(fig1.layout.xaxis)\n",
    "    fig.layout.yaxis.update(fig1.layout.yaxis)\n",
    "    fig.layout.xaxis2.update(fig2.layout.xaxis)\n",
    "    fig.layout.yaxis2.update(fig2.layout.yaxis)\n",
    "    fig.layout.yaxis2.update({'title': {'text': ''}})\n",
    "    \n",
    "    display(scoreLog)\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
